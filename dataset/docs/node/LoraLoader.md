- `LoraLoader`: The LoraLoader node is designed to dynamically load and apply LoRA (Low-Rank Adaptation) adjustments to models and CLIP instances based on specified strengths and LoRA file names. It facilitates the customization of pre-trained models by applying fine-tuned adjustments without altering the original model weights directly, enabling more flexible and targeted model behavior modifications.
    - Parameters:
        - `lora_name`: The name of the LoRA file containing the adjustments to be applied. This enables the selection of specific fine-tuning adjustments for the model and CLIP instance. The specific LoRA file chosen dictates the nature of the adjustments and can lead to varied enhancements or modifications in model performance. Type should be `COMBO[STRING]`.
        - `strength_model`: Determines the intensity of the LoRA adjustments applied to the model. This allows for fine-grained control over the extent of model customization. Higher strengths mean more pronounced adjustments, which can lead to significant changes in model behavior, potentially improving performance on specific tasks. Type should be `FLOAT`.
        - `strength_clip`: Determines the intensity of the LoRA adjustments applied to the CLIP instance. This allows for fine-grained control over the extent of CLIP customization. Similar to the model, higher strengths result in more noticeable changes, affecting how the CLIP model processes data. Type should be `FLOAT`.
    - Inputs:
        - `model`: The model to which LoRA adjustments will be applied. It's crucial for customizing the model's behavior without changing its original structure. The choice of model directly influences the effectiveness and applicability of the LoRA adjustments, as different models may respond differently to the same set of adjustments. Type should be `MODEL`.
        - `clip`: The CLIP instance to which LoRA adjustments will be applied, allowing for customized behavior in processing visual and textual data. The adjustments can significantly alter how the CLIP model processes and interprets visual and textual inputs, thereby affecting the outcomes of tasks like image captioning or text-to-image generation. Type should be `CLIP`.
    - Outputs:
        - `model`: The model with LoRA adjustments applied, reflecting the specified customization. The adjustments can enhance the model's performance on specific tasks or alter its behavior to better suit particular applications. Type should be `MODEL`.
        - `clip`: The CLIP instance with LoRA adjustments applied, reflecting the specified customization. These adjustments can lead to improved or altered performance in tasks involving visual and textual data processing. Type should be `CLIP`.
