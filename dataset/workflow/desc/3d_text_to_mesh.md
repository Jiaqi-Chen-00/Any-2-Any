This workflow begins by loading a checkpoint for a model and generating an empty latent image. Using CLIP Text Encode, it applies both a positive prompt and a negative prompt to exclude unwanted elements. The KSampler module generates an initial latent image from the CLIP-encoded prompts, and the VAE Decode module converts this latent image to a visual format. Afterward, a rembg node removes the background. Notice that the image after rembg needs to be converted to RGB format using a Images to RGB node for further use. A Conver Image to Mask node also is used after background was removed to produce the reference mask. The Triplane Gaussian Transformers model then processes this reference image and the reference mask, creating a 3D geometry with a specified camera distance. The image mask is inverted and transformed with axis switching for proper orientation. The refined 3D structure is then converted into a high-resolution mesh using NeRF and Marching Cubes algorithms, configured with various parameters to control mesh and texture quality. Finally, the generated 3D model is saved as "test.obj" for further use or export.